# DANIEL_AI Orion - Protocolo de Entrenamiento

**Transici√≥n de Sistema Reactivo ‚Üí Ecosistema Proactivo de Triage Multimodal**

---

## üéØ Objetivo

Entrenar y desplegar DANIEL_AI Orion como un sistema de triage inteligente que:
- Opera de forma proactiva con IA m√©dica (Med-Gemma)
- Procesa inputs multimodales (voz, texto, imagen)
- Cumple est√°ndares HIPAA con SafeCore/BioCore
- Optimiza recursos hospitalarios autom√°ticamente
- Aprende continuamente de validaci√≥n cl√≠nica

---

## üìä Arquitectura de Entrenamiento

```mermaid
graph TB
    A[Fase 1:<br/>Calibraci√≥n Sint√©tica] --> B[Fase 2:<br/>Modo Sombra]
    B --> C[Fase 3:<br/>Producci√≥n]
    
    A --> D[1000 casos sint√©ticos]
    A --> E[Validaci√≥n ZKP]
    
    B --> F[Human-in-the-Loop]
    B --> G[6 meses supervisi√≥n]
    B --> H[Multimodal Training]
    
    C --> I[M√©tricas validadas]
    C --> J[Optimizaci√≥n VPP]
    C --> K[Auditor√≠a de sesgos]
    
    style A fill:#4285f4,color:#fff
    style B fill:#fbbc04,color:#000
    style C fill:#34a853,color:#fff
```

---

## üìã Fase 1: Calibraci√≥n con Datos Sint√©ticos

**Duraci√≥n**: 2-4 semanas
**Objetivo**: Validar l√≥gica de triage con datos controlados

### 1.1 Generaci√≥n de Casos Sint√©ticos

**Dataset Inicial**: 1,000 casos sint√©ticos

| Categor√≠a | Casos | Distribuci√≥n |
|-----------|-------|--------------|
| D1 (Emergencia) | 200 | 20% |
| D2 (Urgencia) | 300 | 30% |
| D7 (Baja Complejidad) | 300 | 30% |
| D3 (Consulta) | 200 | 20% |

**Estructura de Caso Sint√©tico**:
```json
{
  "caso_id": "SYNTH_001",
  "sintoma_principal": "dolor toracico",
  "edad": 55,
  "sexo": "M",
  "descripcion": "Dolor opresivo retroesternal de 20 min...",
  "preguntas_respuestas": {
    "¬øInicio brusco?": "si",
    "¬øIrradiaci√≥n?": "brazo izquierdo",
    "¬øDificultad respiratoria?": "si"
  },
  "signos_vitales": {
    "fc": 110,
    "pas": 160,
    "pad": 95,
    "spo2": 94
  },
  "clasificacion_esperada": "D1",
  "diagnostico_real": "Infarto agudo de miocardio"
}
```

### 1.2 Ingesta de Conocimiento (Excel ‚Üí Med-Gemma)

**Proceso**:

1. **Extracci√≥n de Protocolos**:
   ```python
   # Cargar protocolos desde Excel procesado
   knowledge_base = load_json("data/triage_knowledge_base.json")
   
   # Extraer preguntas clave por s√≠ntoma
   for sintoma in knowledge_base:
       preguntas_clave = sintoma["preguntas_obligatorias"]
       conductas = sintoma["reglas_clasificacion"]
   ```

2. **Fine-Tuning de Med-Gemma**:
   ```python
   # Preparar dataset de entrenamiento
   training_data = []
   for caso in casos_sinteticos:
       prompt = build_training_prompt(caso)
       expected_output = caso["clasificacion_esperada"]
       training_data.append((prompt, expected_output))
   
   # Fine-tune Med-Gemma
   model.fine_tune(
       training_data=training_data,
       validation_split=0.2,
       epochs=10
   )
   ```

3. **Validaci√≥n de L√≥gica SET/MTS**:
   - Sistema Espa√±ol de Triaje (SET)
   - Manchester Triage System (MTS)
   - 5 niveles de urgencia

### 1.3 Seguridad: Zero-Knowledge Proof

**Validaci√≥n Obligatoria**:
```python
# Cada caso sint√©tico debe pasar por ZKP
for caso in casos_sinteticos:
    # Generar Bio-Hash
    bio_hash = generate_bio_hash(caso["caso_id"])
    
    # Validar elegibilidad sin exponer datos
    zkp_valid = validate_zkp(bio_hash, caso["clasificacion_esperada"])
    
    if not zkp_valid:
        raise SecurityError("Caso sint√©tico fall√≥ validaci√≥n ZKP")
    
    # Procesar solo si ZKP v√°lido
    resultado = orion.process_triage(caso)
```

### 1.4 M√©tricas de Fase 1

| M√©trica | Objetivo | F√≥rmula |
|---------|----------|---------|
| **Accuracy** | > 95% | Correctos / Total |
| **Precision D1** | > 98% | VP / (VP + FP) |
| **Recall D1** | > 99% | VP / (VP + FN) |
| **F1-Score** | > 0.96 | 2 * (P * R) / (P + R) |

**Criterio de Paso**: Todas las m√©tricas deben cumplir objetivos.

---

## üîÑ Fase 2: Validaci√≥n en Modo Sombra

**Duraci√≥n**: 6 meses m√≠nimo
**Objetivo**: Validaci√≥n cl√≠nica con supervisi√≥n humana

### 2.1 Supervisi√≥n Cl√≠nica (Human-in-the-Loop)

**Arquitectura**:
```
Paciente ‚Üí Orion AI ‚Üí Clasificaci√≥n Sugerida
                              ‚Üì
                      [VALIDACI√ìN OBLIGATORIA]
                              ‚Üì
                    Profesional M√©dico ‚Üí Decisi√≥n Final
                              ‚Üì
                    Feedback Loop ‚Üí Reentrenamiento
```

**Proceso**:

1. **Doble Clasificaci√≥n**:
   ```python
   # Sistema paralelo
   clasificacion_ai = orion.process_triage(caso)
   clasificacion_humana = medico.clasificar(caso)
   
   # Comparar y registrar
   concordancia = (clasificacion_ai.codigo == clasificacion_humana.codigo)
   
   # Log para an√°lisis
   log_shadow_mode(
       caso_id=caso.id,
       ai_classification=clasificacion_ai,
       human_classification=clasificacion_humana,
       concordancia=concordancia,
       tiempo_ai=clasificacion_ai.tiempo,
       tiempo_humano=clasificacion_humana.tiempo
   )
   ```

2. **Validaci√≥n Obligatoria**:
   - ‚úÖ M√©dico revisa **TODAS** las sugerencias de IA
   - ‚úÖ Decisi√≥n final siempre es humana
   - ‚úÖ IA solo sugiere, no ejecuta

3. **Feedback Loop**:
   ```python
   # Cuando hay discordancia
   if not concordancia:
       # Capturar raz√≥n de discordancia
       razon = medico.explicar_diferencia()
       
       # Agregar a dataset de reentrenamiento
       training_cases.append({
           "caso": caso,
           "clasificacion_ai": clasificacion_ai,
           "clasificacion_correcta": clasificacion_humana,
           "razon_error": razon
       })
       
       # Reentrenar semanalmente
       if len(training_cases) >= 100:
           retrain_model(training_cases)
   ```

### 2.2 Refinamiento Multimodal (Project Astra)

**Capacidades a Entrenar**:

#### A. Audio (Voz del Paciente)
```python
# Transcripci√≥n + An√°lisis de Tono
audio_input = capture_audio(paciente)
transcripcion = speech_to_text(audio_input)
tono_urgencia = analyze_voice_urgency(audio_input)

# Combinar con clasificaci√≥n
resultado = orion.process_triage(
    input_text=transcripcion,
    audio_features={"urgencia_tono": tono_urgencia}
)
```

#### B. Visual (Fotos de Lesiones)
```python
# An√°lisis de imagen m√©dica
imagen = capture_image(lesion)
caracteristicas = analyze_medical_image(imagen)

# Integrar con triage
resultado = orion.process_triage(
    input_text="Lesi√≥n cut√°nea",
    visual_features={
        "tipo_lesion": caracteristicas.tipo,
        "severidad_visual": caracteristicas.severidad,
        "area_afectada": caracteristicas.area
    }
)
```

#### C. Texto (Descripci√≥n Escrita)
```python
# Ya implementado en Orion
texto = paciente.describir_sintomas()
resultado = orion.process_triage(input_text=texto)
```

#### D. Fusi√≥n Multimodal
```python
# Combinar todas las modalidades
resultado_final = orion.multimodal_triage(
    audio=audio_input,
    visual=imagen,
    texto=texto,
    biometria=signos_vitales
)

# Ponderaci√≥n por confianza
confianza_final = (
    0.3 * resultado_audio.confianza +
    0.3 * resultado_visual.confianza +
    0.2 * resultado_texto.confianza +
    0.2 * resultado_biometria.confianza
)
```

### 2.3 Integraci√≥n BioCore

**Calibraci√≥n con Signos Vitales Reales**:

```python
class BiometricIntegration:
    """Integraci√≥n segura con dispositivos biom√©tricos"""
    
    THRESHOLDS = {
        "fc_taquicardia": 100,
        "fc_bradicardia": 60,
        "pas_hipertension": 140,
        "spo2_hipoxia": 90,
        "temp_fiebre": 38.0
    }
    
    def calibrate_with_vitals(self, caso, biometria):
        """Ajusta clasificaci√≥n seg√∫n signos vitales"""
        
        # Detectar alertas biom√©tricas
        alertas = []
        
        if biometria.heart_rate > self.THRESHOLDS["fc_taquicardia"]:
            alertas.append("TAQUICARDIA")
        
        if biometria.oxygen_saturation < self.THRESHOLDS["spo2_hipoxia"]:
            alertas.append("HIPOXIA")
        
        # Escalar clasificaci√≥n si hay alertas cr√≠ticas
        if "HIPOXIA" in alertas or "SHOCK" in alertas:
            if caso.clasificacion != "D1":
                caso.clasificacion = "D1"
                caso.razon_escalamiento = f"Escalado por: {', '.join(alertas)}"
        
        # Vincular de forma segura (HIPAA)
        bio_hash = generate_bio_hash(
            patient_id=caso.patient_id,
            biometric_data=biometria
        )
        
        return caso, bio_hash
```

### 2.4 M√©tricas de Fase 2

| M√©trica | Objetivo | Medici√≥n |
|---------|----------|----------|
| **Concordancia AI-Humano** | > 90% | Casos coincidentes / Total |
| **Tiempo Promedio AI** | < 2 seg | Latencia de clasificaci√≥n |
| **Tiempo Promedio Humano** | Baseline | Comparaci√≥n de eficiencia |
| **Casos Escalados Correctamente** | > 95% | D3‚ÜíD1 justificados |
| **Satisfacci√≥n M√©dicos** | > 4/5 | Encuesta mensual |

---

## üöÄ Fase 3: Despliegue Operativo y Aprendizaje Continuo

**Duraci√≥n**: Indefinida (producci√≥n)
**Objetivo**: Sistema aut√≥nomo con supervisi√≥n reducida

### 3.1 Validaci√≥n de M√©tricas Pre-Producci√≥n

**Criterios de Paso a Producci√≥n**:

#### A. Error Absoluto Medio (MAE)
```python
# Calcular MAE en escala de urgencia
def calculate_mae(predictions, actuals):
    urgencia_map = {"D1": 4, "D2": 3, "D7": 2, "D3": 1}
    
    errors = []
    for pred, actual in zip(predictions, actuals):
        error = abs(urgencia_map[pred] - urgencia_map[actual])
        errors.append(error)
    
    mae = sum(errors) / len(errors)
    return mae

# Objetivo: MAE < 0.15 (muy bajo error)
```

#### B. Sensibilidad (Recall) para D1
```python
# Sensibilidad = VP / (VP + FN)
# Objetivo: > 99% (no perder emergencias)

def calculate_sensitivity_d1(predictions, actuals):
    vp = sum(1 for p, a in zip(predictions, actuals) 
             if p == "D1" and a == "D1")
    fn = sum(1 for p, a in zip(predictions, actuals) 
             if p != "D1" and a == "D1")
    
    sensitivity = vp / (vp + fn) if (vp + fn) > 0 else 0
    return sensitivity
```

#### C. Especificidad para D3
```python
# Especificidad = VN / (VN + FP)
# Objetivo: > 85% (no saturar con consultas)

def calculate_specificity_d3(predictions, actuals):
    vn = sum(1 for p, a in zip(predictions, actuals) 
             if p != "D3" and a != "D3")
    fp = sum(1 for p, a in zip(predictions, actuals) 
             if p == "D3" and a != "D3")
    
    specificity = vn / (vn + fp) if (vn + fp) > 0 else 0
    return specificity
```

**Dashboard de M√©tricas**:
```python
def production_readiness_report():
    return {
        "mae": calculate_mae(predictions, actuals),
        "sensitivity_d1": calculate_sensitivity_d1(predictions, actuals),
        "specificity_d3": calculate_specificity_d3(predictions, actuals),
        "accuracy": calculate_accuracy(predictions, actuals),
        "f1_score": calculate_f1(predictions, actuals),
        "ready_for_production": all([
            mae < 0.15,
            sensitivity_d1 > 0.99,
            specificity_d3 > 0.85,
            accuracy > 0.95
        ])
    }
```

### 3.2 Optimizaci√≥n de Recursos (VPP)

**Identificaci√≥n Autom√°tica de Casos VPP**:

```python
class VPPOptimizer:
    """Optimizador de V√≠as de Procesamiento Vertical"""
    
    VPP_ELIGIBLE_CODES = ["D7", "D3"]
    
    def suggest_vpp(self, caso, clasificacion):
        """Sugiere derivaci√≥n a VPP para liberar recursos"""
        
        if clasificacion.codigo not in self.VPP_ELIGIBLE_CODES:
            return None
        
        # Criterios adicionales para VPP
        criterios_vpp = {
            "sin_comorbilidades_graves": True,
            "signos_vitales_estables": self._check_vitals_stable(caso.biometria),
            "puede_esperar_4h": True,
            "no_requiere_procedimientos_urgentes": True
        }
        
        if all(criterios_vpp.values()):
            return {
                "derivar_a_vpp": True,
                "razon": "Baja complejidad, signos vitales estables",
                "tiempo_espera_estimado": "2-4 horas",
                "recursos_liberados": ["Cama urgencias", "Personal enfermer√≠a"]
            }
        
        return None
    
    def calculate_resource_impact(self, vpp_cases_per_day):
        """Calcula impacto en recursos hospitalarios"""
        
        # Estimaciones
        tiempo_promedio_urgencias = 3  # horas
        tiempo_promedio_vpp = 1.5  # horas
        
        horas_liberadas = vpp_cases_per_day * (
            tiempo_promedio_urgencias - tiempo_promedio_vpp
        )
        
        return {
            "casos_vpp_diarios": vpp_cases_per_day,
            "horas_liberadas_diarias": horas_liberadas,
            "capacidad_adicional_d1_d2": horas_liberadas / 3,  # casos adicionales
            "ahorro_mensual_cop": horas_liberadas * 30 * 50000  # estimado
        }
```

### 3.3 Auditor√≠a de Sesgos

**An√°lisis Sistem√°tico de Equidad**:

```python
class BiasAuditor:
    """Auditor de sesgos algor√≠tmicos"""
    
    PROTECTED_ATTRIBUTES = ["edad", "sexo", "etnia", "nivel_socioeconomico"]
    
    def audit_fairness(self, predictions, demographics):
        """Audita equidad en clasificaciones"""
        
        report = {}
        
        for attribute in self.PROTECTED_ATTRIBUTES:
            # Agrupar por atributo protegido
            groups = self._group_by_attribute(demographics, attribute)
            
            # Calcular m√©tricas por grupo
            for group_name, group_cases in groups.items():
                group_predictions = [predictions[i] for i in group_cases]
                
                report[f"{attribute}_{group_name}"] = {
                    "total_casos": len(group_cases),
                    "d1_rate": self._calculate_rate(group_predictions, "D1"),
                    "d2_rate": self._calculate_rate(group_predictions, "D2"),
                    "tiempo_promedio": self._avg_time(group_cases)
                }
        
        # Detectar disparidades
        disparities = self._detect_disparities(report)
        
        return {
            "report": report,
            "disparities_detected": disparities,
            "action_required": len(disparities) > 0
        }
    
    def _detect_disparities(self, report):
        """Detecta disparidades significativas entre grupos"""
        
        disparities = []
        
        # Comparar tasas de D1 entre grupos
        d1_rates = {k: v["d1_rate"] for k, v in report.items()}
        max_rate = max(d1_rates.values())
        min_rate = min(d1_rates.values())
        
        # Si diferencia > 10%, hay disparidad
        if (max_rate - min_rate) > 0.10:
            disparities.append({
                "type": "D1_classification_disparity",
                "max_group": max(d1_rates, key=d1_rates.get),
                "min_group": min(d1_rates, key=d1_rates.get),
                "difference": max_rate - min_rate
            })
        
        return disparities
```

### 3.4 Aprendizaje Continuo

**Reentrenamiento Autom√°tico**:

```python
class ContinuousLearning:
    """Sistema de aprendizaje continuo"""
    
    def __init__(self):
        self.feedback_buffer = []
        self.retrain_threshold = 1000  # casos
        self.retrain_frequency = "weekly"
    
    def collect_feedback(self, caso, clasificacion_ai, clasificacion_real):
        """Recolecta feedback de casos reales"""
        
        self.feedback_buffer.append({
            "caso": caso,
            "ai_prediction": clasificacion_ai,
            "ground_truth": clasificacion_real,
            "timestamp": datetime.now(),
            "error": clasificacion_ai.codigo != clasificacion_real.codigo
        })
        
        # Reentrenar si se alcanza umbral
        if len(self.feedback_buffer) >= self.retrain_threshold:
            self.trigger_retraining()
    
    def trigger_retraining(self):
        """Dispara proceso de reentrenamiento"""
        
        print(f"üîÑ Iniciando reentrenamiento con {len(self.feedback_buffer)} casos")
        
        # Preparar dataset
        training_data = self._prepare_training_data(self.feedback_buffer)
        
        # Fine-tune incremental
        model.incremental_fine_tune(
            new_data=training_data,
            learning_rate=0.0001,  # bajo para no olvidar conocimiento previo
            epochs=5
        )
        
        # Validar nuevo modelo
        metrics = self._validate_new_model()
        
        if metrics["accuracy"] > 0.95:
            print("‚úÖ Nuevo modelo validado - Desplegando")
            self.deploy_new_model()
        else:
            print("‚ö†Ô∏è Nuevo modelo no cumple m√©tricas - Revertiendo")
        
        # Limpiar buffer
        self.feedback_buffer = []
```

---

## üìä Cronograma de Implementaci√≥n

| Fase | Duraci√≥n | Hitos Clave |
|------|----------|-------------|
| **Fase 1** | 2-4 semanas | 1000 casos sint√©ticos, M√©tricas > 95% |
| **Fase 2** | 6 meses | Concordancia > 90%, Multimodal OK |
| **Fase 3** | Ongoing | MAE < 0.15, Producci√≥n estable |

---

## ‚úÖ Checklist de Validaci√≥n

### Pre-Fase 1
- [ ] Dataset sint√©tico de 1000 casos generado
- [ ] Protocolos Excel cargados en knowledge base
- [ ] SafeCore ZKP configurado
- [ ] M√©tricas baseline establecidas

### Pre-Fase 2
- [ ] Accuracy > 95% en casos sint√©ticos
- [ ] Precision D1 > 98%
- [ ] Recall D1 > 99%
- [ ] Sistema de feedback implementado

### Pre-Fase 3
- [ ] 6 meses de modo sombra completados
- [ ] Concordancia AI-Humano > 90%
- [ ] Multimodal (audio/visual/texto) funcional
- [ ] BioCore integrado y validado
- [ ] MAE < 0.15
- [ ] Sensibilidad D1 > 99%
- [ ] Especificidad D3 > 85%
- [ ] Auditor√≠a de sesgos sin disparidades

---

**DANIEL_AI Orion** - De Reactivo a Proactivo üöÄ
